from typing import Dict, List

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score


# Function to visualize the distribution of particle features (pT, eta, phi, E)
def plot_feature_distribution(X_jets: np.ndarray) -> None:
    feature_names = ['pT', 'eta', 'phi', 'energy']
    plt.figure(figsize=(12, 8))

    for i, feature in enumerate(feature_names):
        plt.subplot(2, 2, i + 1)
        sns.histplot(X_jets[:, i], bins=50, kde=True)
        plt.xlabel(feature)
        plt.ylabel("Frequency")
        plt.grid(True)

    plt.tight_layout()
    plt.show()


# Function to visualize the particle features reconstruction
def plot_particle_reconstruction(y_true: np.ndarray, y_pred: np.ndarray) -> None:
    pT_true, eta_true, phi_true, E_true = y_true[:, 0], y_true[:, 1], y_true[:, 2], y_true[:, 3]
    pT_pred, eta_pred, phi_pred, E_pred = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2], y_pred[:, 3]

    plt.figure(figsize=(12, 10))

    # pT histogram
    pT_min = min(pT_true.min(), pT_pred.min())
    pT_max = max(pT_true.max(), pT_pred.max())
    plt.subplot(2, 2, 1)
    plt.hist2d(pT_true, pT_pred, bins=50, cmap='gist_heat_r')
    plt.xlabel("true scaled pT")
    plt.ylabel("predicted scaled pT")
    plt.title("scaled pT distribution")
    plt.colorbar()
    plt.plot([pT_min, pT_max], [pT_min, pT_max], color='blue', linestyle='-')
    plt.xlim(pT_min, pT_max)
    plt.ylim(pT_min, pT_max)
    
    # Eta histogram
    eta_min = min(eta_true.min(), eta_pred.min())
    eta_max = max(eta_true.max(), eta_pred.max())
    plt.subplot(2, 2, 2)
    plt.hist2d(eta_true, eta_pred, bins=50, cmap='gist_heat_r')
    plt.xlabel("true eta")
    plt.ylabel("predicted eta")
    plt.title("eta distribution")
    plt.colorbar()
    plt.plot([eta_min, eta_max], [eta_min, eta_max], color='blue', linestyle='-')
    plt.xlim(eta_min, eta_max)
    plt.ylim(eta_min, eta_max)
    
    # Phi histogram
    phi_min = min(phi_true.min(), phi_pred.min())
    phi_max = max(phi_true.max(), phi_pred.max())
    plt.subplot(2, 2, 3)
    plt.hist2d(phi_true, phi_pred, bins=50, cmap='gist_heat_r')
    plt.xlabel("true phi")
    plt.ylabel("predicted phi")
    plt.title("phi distribution")
    plt.colorbar()
    plt.plot([phi_min, phi_max], [phi_min, phi_max], color='blue', linestyle='-')
    plt.xlim(phi_min, phi_max)
    plt.ylim(phi_min, phi_max)

    # Energy histogram
    E_min = min(E_true.min(), E_pred.min())
    E_max = max(E_true.max(), E_pred.max())
    plt.subplot(2, 2, 4)
    plt.hist2d(E_true, E_pred, bins=50, cmap='gist_heat_r')
    plt.xlabel("true scaled energy")
    plt.ylabel("predicted scaled energy")
    plt.title("scaled energy distribution")
    plt.colorbar()
    plt.plot([E_min, E_max], [E_min, E_max], color='blue', linestyle='-')
    plt.xlim(E_min, E_max)
    plt.ylim(E_min, E_max)
    
    plt.tight_layout()
    plt.show()


# Function to visualize the training progress
def plot_history(history: Dict[str, List[float]]) -> None:
    plt.figure(figsize=(12, 5))
    epochs = history['epoch']

    # Plot training and validation loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['train_loss'], label="Train Loss")
    plt.plot(epochs, history['val_loss'], label="Validation Loss")
    plt.title("Training and Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # Plot training and validation metric (accuracy, for example)
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['train_metric'], label="Train Accuracy")
    plt.plot(epochs, history['val_metric'], label="Validation Accuracy")
    plt.title("Training and Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()


# Function to visualize the confusion matrix
def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, labels: List[str]) -> None:
    y_true_classes = np.argmax(y_true, axis=1)
    y_pred_classes = np.argmax(y_pred, axis=1)
    cm = confusion_matrix(y_true_classes, y_pred_classes,labels=np.arange(y_true.shape[1]))

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=labels, yticklabels=labels)
    plt.yticks(rotation=0)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.show()


# Function to visualize the ROC curve
def plot_roc_curve(y_true: np.ndarray, y_pred_prob: np.ndarray) -> None:
    # Convert one-hot encoded y_true to class indices if needed
    if y_true.ndim > 1 and y_true.shape[1] > 1:
        y_true_indices = np.argmax(y_true, axis=1)
    else:
        y_true_indices = y_true
    
    # Get number of classes
    n_classes = y_pred_prob.shape[1]
    
    # Compute macro-average ROC AUC score
    roc_auc = roc_auc_score(y_true, y_pred_prob, average='macro', multi_class='ovo')
    
    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    for i in range(n_classes):
        # For each class, get binary indicators
        y_true_binary = (y_true_indices == i).astype(int)
        y_score = y_pred_prob[:, i]
        
        # Calculate ROC curve for this class
        fpr[i], tpr[i], _ = roc_curve(y_true_binary, y_score)
    
    # Compute macro-average ROC curve by interpolating and averaging
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(n_classes):
        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
    
    # Average and compute AUC
    mean_tpr /= n_classes
    
    # Plot the macro-average ROC curve
    plt.figure(figsize=(6, 5))
    plt.plot(all_fpr, mean_tpr, color='orange', label=f"AUC = {roc_auc:.3f}")
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Macro-Average ROC Curve")
    plt.legend(loc='lower right')
    plt.tight_layout()
    plt.show()


# Function to visualize pretraining loss for VICReg
def plot_VICReg_history(history: Dict[str, List[float]]) -> None:
    # Find the epoch where the VICReg loss is minimal
    min_epoch = history['VICReg_loss'].index(min(history['VICReg_loss']))

    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history['variance_loss'], label="variance loss")
    plt.plot(history['invariance_loss'], label="invariance loss")
    plt.plot(history['covariance_loss'], label="covariance loss")
    plt.axvline(x=min_epoch, color='black', linestyle='--', label="min VICReg loss")
    plt.title("Pretraining Losses")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(history['VICReg_loss'], color='red', label="VICReg loss")
    plt.axvline(x=min_epoch, color='black', linestyle='--', label="min VICReg loss")
    plt.title("Total VICReg Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()